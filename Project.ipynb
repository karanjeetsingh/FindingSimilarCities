{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Similar Neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Twitter data grabber - \n",
    "This code listens to twitter stream for live foursquare checkins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Retrieve checkins tweets\n",
    "\n",
    "    We use mongodb to save the tweets\n",
    "    Mongodb is hosted on local machine and at port 27017\n",
    "    \n",
    "    This method runs for 1 hour and then saves all the tweets found in the mongodb\n",
    "\"\"\"\n",
    "from timeit import default_timer as clock\n",
    "from time import sleep\n",
    "import TwitterAPI as twitter\n",
    "import twitter_helper as th\n",
    "import arguments\n",
    "import ConfigParser\n",
    "DB = None\n",
    "SAVE = None\n",
    "import CommonMongo as cm\n",
    "DB = cm.connect_to_db('foursquare', 'localhost', 27017)[0]\n",
    "\n",
    "import CheckinAPICrawler as cac\n",
    "CRAWLER = cac.CheckinAPICrawler()\n",
    "from Queue import Queue\n",
    "from threading import Thread\n",
    "# the size of mongo bulk insert, in multiple of pool size\n",
    "INSERT_SIZE = 7\n",
    "CHECKINS_QUEUE = Queue((INSERT_SIZE+3)*cac.BITLY_SIZE)\n",
    "NB_TWEETS = 0\n",
    "NUM_VALID = 0\n",
    "\n",
    "\n",
    "def post_process(checkins):\n",
    "    \"\"\"use `crawler` to follow URL within `checkins` and update them with\n",
    "    information regarding the actual Foursquare checkin.\"\"\"\n",
    "    infos = CRAWLER.checkins_from_url([c.lid for c in checkins])\n",
    "    if not infos:\n",
    "        return None\n",
    "    finalized = []\n",
    "    global NUM_VALID\n",
    "    for checkin, info in zip(checkins, infos):\n",
    "        if info:\n",
    "            converted = checkin._asdict()\n",
    "            id_, uid, vid, time = info\n",
    "            del converted['id']\n",
    "            converted['_id'] = id_\n",
    "            converted['uid'] = uid\n",
    "            converted['lid'] = vid\n",
    "            converted['time'] = time\n",
    "            finalized.append(converted)\n",
    "            NUM_VALID += 1\n",
    "        CHECKINS_QUEUE.task_done()\n",
    "    return finalized\n",
    "\n",
    "\n",
    "def accumulate_checkins():\n",
    "    \"\"\"Call save checkin as soon as a batch is complete (or the last one was\n",
    "    received).\"\"\"\n",
    "    waiting_for_crawling = []\n",
    "    while True:\n",
    "        checkin = CHECKINS_QUEUE.get()\n",
    "        if not checkin:\n",
    "            # receive None, signaling end of the time allowed\n",
    "            CHECKINS_QUEUE.task_done()\n",
    "            break\n",
    "        waiting_for_crawling.append(checkin)\n",
    "        if len(waiting_for_crawling) == INSERT_SIZE*cac.BITLY_SIZE:\n",
    "            status = save_checkins(post_process(waiting_for_crawling), SAVE)\n",
    "            del waiting_for_crawling[:]\n",
    "            # status is None when CRAWLER.checkins_from_url returns None\n",
    "            if status is None:\n",
    "                break\n",
    "    save_checkins(post_process(waiting_for_crawling), SAVE)\n",
    "\n",
    "\n",
    "def save_checkins(complete, saving_method):\n",
    "    \"\"\"Save `complete` using `saving_method`.\"\"\"\n",
    "    print 'Save `complete` using `saving_method`.'\n",
    "    if not complete:\n",
    "        return None\n",
    "    saving_method(complete)\n",
    "    return True\n",
    "\n",
    "\n",
    "def save_checkins_mongo(complete):\n",
    "    \"\"\"Save `complete` in DB.\"\"\"\n",
    "    try:\n",
    "        DB.checkin.insert(complete, continue_on_error=True)\n",
    "        print('insert {}'.format(len(complete)))\n",
    "    except cm.pymongo.errors.DuplicateKeyError:\n",
    "        pass\n",
    "    except cm.pymongo.errors.OperationFailure as err:\n",
    "        print(err, err.code)\n",
    "\n",
    "\n",
    "def read_twitter_stream(client, end, logging_step=60):\n",
    "    \"\"\"Iterate over tweets and put those matched by parse_tweet in a queue,\n",
    "    until current time is more than `end`. Log info every `logging_step` new\n",
    "    valid candidate.\"\"\"\n",
    "    global NB_TWEETS\n",
    "    req = client.request('statuses/filter', {'track': '4sq,swarmapp'})\n",
    "    new_tweet = 'get {}, {}/{}, {:.1f} seconds to go'\n",
    "    nb_cand = 0\n",
    "    for item in req.get_iterator():\n",
    "        candidate = th.parse_tweet(item)\n",
    "        NB_TWEETS += 1\n",
    "        if candidate:\n",
    "            #print candidate\n",
    "            CHECKINS_QUEUE.put_nowait(candidate)\n",
    "            nb_cand += 1\n",
    "            if nb_cand % logging_step == 0:\n",
    "                cac.logging.info(new_tweet.format(candidate.tid,\n",
    "                                                  nb_cand, NB_TWEETS,\n",
    "                                                  end - clock()))\n",
    "            if clock() >= end:\n",
    "                CHECKINS_QUEUE.put_nowait(None)\n",
    "                break\n",
    "\n",
    "if DB:\n",
    "    DB.checkin.ensure_index([('loc', cm.pymongo.GEOSPHERE),\n",
    "                             ('lid', cm.pymongo.ASCENDING),\n",
    "                             ('city', cm.pymongo.ASCENDING),\n",
    "                             ('time', cm.pymongo.ASCENDING)])\n",
    "SAVE = save_checkins_mongo\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.read('api_keys.cfg')\n",
    "api = twitter.TwitterAPI(config.get('twitter', 'consumer_key'),\n",
    "               config.get('twitter', 'consumer_secret'),\n",
    "               config.get('twitter', 'access_token'),\n",
    "               config.get('twitter', 'access_token_secret'))\n",
    "accu = Thread(target=accumulate_checkins, name='AccumulateCheckins')\n",
    "accu.daemon = True\n",
    "accu.start()\n",
    "start = clock()\n",
    "end = start + 1*60*60 # 1 hour\n",
    "failures = th.Failures(initial_waiting_time=2.0)\n",
    "while clock() < end:\n",
    "    print clock()\n",
    "    try:\n",
    "        read_twitter_stream(api, end)\n",
    "        print 'stream read'\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        CHECKINS_QUEUE.put_nowait(None)\n",
    "        print 'keyboard interrupt'\n",
    "        raise\n",
    "    except:\n",
    "        msg = 'Fail to read or enqueue tweet\\n'\n",
    "        #print msg\n",
    "        cac.logging.exception(msg)\n",
    "        waiting_time = failures.fail()\n",
    "        if clock() + waiting_time > end or \\\n",
    "           failures.recent_failures >= 5 or \\\n",
    "           not accu.is_alive():\n",
    "            CHECKINS_QUEUE.put_nowait(None)\n",
    "            break\n",
    "        msg = 'Will wait for {:.0f} seconds'.format(waiting_time)\n",
    "        cac.logging.info(msg)\n",
    "        failures.do_sleep()\n",
    "\n",
    "CHECKINS_QUEUE.join()\n",
    "report = 'insert {} valid checkins in {:.2f}s (out of {}).'\n",
    "cac.logging.info(report.format(NUM_VALID, clock() - start, NB_TWEETS))\n",
    "sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the database with foursquare data\n",
    "Use collected tweet and AskFourquare to request information about users and venues, before inserting them in a Mongo database\n",
    "\n",
    "We need to specify the city before we can fetch foursquare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Use collected tweet and AskFourquare to fill user and venue table in the\n",
    "Database.\"\"\"\n",
    "from time import sleep\n",
    "from threading import Thread\n",
    "import foursquare\n",
    "import CommonMongo as cm\n",
    "import Chunker\n",
    "from Queue import Queue\n",
    "from RequestsMonitor import RequestsMonitor\n",
    "from AskFourquare import gather_all_entities_id\n",
    "from AskFourquare import user_profile, venue_profile\n",
    "import sys\n",
    "import arguments\n",
    "import ssl\n",
    "import ConfigParser\n",
    "\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.read('api_keys.cfg')\n",
    "CLIENT_ID = config.get('foursquare', 'FOURSQUARE_ID2')\n",
    "CLIENT_SECRET = config.get('foursquare', 'FOURSQUARE_SECRET2')\n",
    "\n",
    "ENTITY_KIND = 'venue'\n",
    "if ENTITY_KIND == 'venue':\n",
    "    RATE = 5000\n",
    "    REQ = 'venues'\n",
    "    DB_FIELD = 'lid'\n",
    "    PARSE = venue_profile\n",
    "elif ENTITY_KIND == 'user':\n",
    "    RATE = 500\n",
    "    REQ = 'users'\n",
    "    DB_FIELD = 'uid'\n",
    "    PARSE = user_profile\n",
    "else:\n",
    "    raise ValueError(ENTITY_KIND + ' is unknown')\n",
    "CLIENT = foursquare.Foursquare(CLIENT_ID, CLIENT_SECRET)\n",
    "IDS_QUEUE = Queue(5)\n",
    "ENTITIES_QUEUE = Queue(105)\n",
    "LIMITOR = RequestsMonitor(RATE)\n",
    "TABLE = []\n",
    "TO_BE_INSERTED = []\n",
    "INVALID_ID = []\n",
    "\n",
    "\n",
    "def convert_entity_for_mongo(entity):\n",
    "    suitable = entity._asdict()\n",
    "    suitable['_id'] = suitable['id']\n",
    "    del suitable['id']\n",
    "    return suitable\n",
    "\n",
    "\n",
    "def entities_getter():\n",
    "    foursquare_is_down = False\n",
    "    while True:\n",
    "        batch = IDS_QUEUE.get()\n",
    "        if foursquare_is_down:\n",
    "            IDS_QUEUE.task_done()\n",
    "            continue\n",
    "        go, wait = LIMITOR.more_allowed(CLIENT)\n",
    "        if not go:\n",
    "            sleep(wait + 3)\n",
    "        for id_ in batch:\n",
    "            REQ(id_, multi=True)\n",
    "        answers = []\n",
    "        try:\n",
    "            answers = list(CLIENT.multi())\n",
    "        except foursquare.ParamError as e:\n",
    "            print(e)\n",
    "            invalid = str(e).split('/')[-1].replace(' ', '+')\n",
    "            answers = individual_query(batch, invalid)\n",
    "        except foursquare.ServerError as e:\n",
    "            print(e)\n",
    "            foursquare_is_down = True\n",
    "        except ssl.SSLError as e:\n",
    "            print(e)\n",
    "            foursquare_is_down = True\n",
    "\n",
    "        for a in answers:\n",
    "            dispatch_answer(a)\n",
    "        IDS_QUEUE.task_done()\n",
    "\n",
    "\n",
    "def dispatch_answer(a):\n",
    "    \"\"\"According to the type of answer of `a`, either enqueue it for DB\n",
    "    insertion or try to save its id as invalid.\"\"\"\n",
    "    if a is None:\n",
    "        print('None answer')\n",
    "    elif not isinstance(a, foursquare.FoursquareException):\n",
    "        parsed = PARSE(a[ENTITY_KIND])\n",
    "        if parsed:\n",
    "            ENTITIES_QUEUE.put(parsed)\n",
    "        else:\n",
    "            # no lon and lat\n",
    "            vid = a[ENTITY_KIND].get('id')\n",
    "            if vid:\n",
    "                INVALID_ID.append(vid)\n",
    "    else:\n",
    "        print(a)\n",
    "        # deleted venue or server error\n",
    "        potential_id = str(a).split()[1]\n",
    "        if len(potential_id) == 24:\n",
    "            INVALID_ID.append(potential_id)\n",
    "\n",
    "\n",
    "def individual_query(batch, invalid):\n",
    "    print(batch, invalid)\n",
    "    answers = []\n",
    "    for id_ in batch:\n",
    "        a = None\n",
    "        if id_ != invalid:\n",
    "            try:\n",
    "                a = REQ(id_)\n",
    "            except (KeyboardInterrupt, SystemExit):\n",
    "                raise\n",
    "            except:\n",
    "                print(sys.exc_info()[1])\n",
    "        answers.append(a)\n",
    "    assert len(answers) == len(batch)\n",
    "    return answers\n",
    "\n",
    "\n",
    "def entities_putter():\n",
    "    while True:\n",
    "        entity = ENTITIES_QUEUE.get()\n",
    "        TO_BE_INSERTED.append(convert_entity_for_mongo(entity))\n",
    "        if len(TO_BE_INSERTED) >= 400:\n",
    "            mongo_insertion()\n",
    "        ENTITIES_QUEUE.task_done()\n",
    "\n",
    "\n",
    "def mongo_insertion():\n",
    "    global TO_BE_INSERTED\n",
    "    if len(TO_BE_INSERTED) == 0:\n",
    "        return\n",
    "    try:\n",
    "        TABLE.insert(TO_BE_INSERTED, continue_on_error=True)\n",
    "    except cm.pymongo.errors.DuplicateKeyError:\n",
    "        pass\n",
    "    except cm.pymongo.errors.OperationFailure as e:\n",
    "        print(e, e.code)\n",
    "    del TO_BE_INSERTED[:]\n",
    "\n",
    "\n",
    "REQ = getattr(CLIENT, REQ)\n",
    "db = cm.connect_to_db('foursquare', '127.0.0.1', 27017)[0]\n",
    "checkins = db['checkin']\n",
    "TABLE = db[ENTITY_KIND]\n",
    "if ENTITY_KIND == 'venue':\n",
    "    TABLE.ensure_index([('loc', cm.pymongo.GEOSPHERE),\n",
    "                        ('city', cm.pymongo.ASCENDING),\n",
    "                        ('cat', cm.pymongo.ASCENDING)])\n",
    "t = Thread(target=entities_getter, name='Query4SQ')\n",
    "t.daemon = True\n",
    "t.start()\n",
    "t = Thread(target=entities_putter, name='InsertDB')\n",
    "t.daemon = True\n",
    "t.start()\n",
    "total_entities = 0\n",
    "\n",
    "city = 'moscow' #Specify city here\n",
    "\n",
    "chunker = Chunker.Chunker(foursquare.MAX_MULTI_REQUESTS)\n",
    "previous = [e['_id'] for e in TABLE.find({'city': city})]\n",
    "print previous\n",
    "potential = gather_all_entities_id(checkins, DB_FIELD, city=city)\n",
    "print('but already {} {}s in DB.'.format(len(previous), ENTITY_KIND))\n",
    "import persistent as p\n",
    "region = city or 'world'\n",
    "invalid_filename = 'non_{}_id_{}'.format(ENTITY_KIND, region)\n",
    "try:\n",
    "    INVALID_ID = p.load_var(invalid_filename)\n",
    "except IOError:\n",
    "    pass\n",
    "print('and {} {}s are invalid.'.format(len(INVALID_ID), ENTITY_KIND))\n",
    "new_ones = set(potential).difference(set(previous))\n",
    "new_ones = new_ones.difference(set(INVALID_ID))\n",
    "outside = set([e['_id'] for e in TABLE.find({'city': None}, {'_id': 1})])\n",
    "outside.intersection_update(new_ones)\n",
    "print('and {} {}s are outside range.'.format(len(outside), ENTITY_KIND))\n",
    "new_ones = new_ones.difference(outside)\n",
    "print('So only {} new ones.'.format(len(new_ones)))\n",
    "for batch in chunker(new_ones):\n",
    "    IDS_QUEUE.put(batch)\n",
    "    total_entities += len(batch)\n",
    "\n",
    "IDS_QUEUE.join()\n",
    "ENTITIES_QUEUE.join()\n",
    "mongo_insertion()\n",
    "print('{}/{} invalid id'.format(len(INVALID_ID), total_entities))\n",
    "print('{}/{} requests'.format(CLIENT.rate_remaining, CLIENT.rate_limit))\n",
    "p.save_var(invalid_filename, INVALID_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintain a tree of foursquare categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persistent.py/load_var\n",
      "CommonMongo.py/connect_to_db\n",
      "CommonMongo.py/build_query\n",
      "[{u'_id': u'34100109', u'checkins': 1}]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Weird id: 54106aaa498e543ccf8c8eff?s=-ACF6zGAPW4VJ5gmNNq4YVjgFh8\n",
      "Invalid checkin id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[u'34100109']\n",
      "persistent.py/load_var\n",
      "persistent.py/load_var\n",
      "persistent.py/load_var\n",
      "[u'34100109']\n",
      "Still 1 users to process.\n",
      "34100109\n",
      "persistent.py/save_var\n",
      "persistent.py/save_var\n",
      "persistent.py/save_var\n",
      "1080\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Maintain a tree of Foursquare categories and provide query methods.\"\"\"\n",
    "from collections import namedtuple\n",
    "import persistent as p\n",
    "import string\n",
    "Category = namedtuple('Category', ['id', 'name', 'depth', 'sub'])\n",
    "import enum\n",
    "Field = enum.Enum('Field', 'id name')  # pylint: disable=C0103\n",
    "import bidict\n",
    "CAT_TO_ID = bidict.bidict({None: '0', 'Venue': '1'})\n",
    "ID_TO_INDEX = bidict.bidict({None: 0, '0': 0, '1': 1})\n",
    "\n",
    "\n",
    "def parse_categories(top_list, depth=0):\n",
    "    \"\"\"Recursively build Categories\"\"\"\n",
    "    if len(top_list) == 0:\n",
    "        return []\n",
    "    res = []\n",
    "    for cat in top_list:\n",
    "        subs = []\n",
    "        if isinstance(cat, dict) and 'categories' in cat:\n",
    "            subs = parse_categories(cat['categories'], depth+1)\n",
    "        id_, name = str(cat['id']), unicode(cat['name'])\n",
    "        CAT_TO_ID[name] = id_\n",
    "        res.append(Category(id_, name, depth+1, subs))\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_categories(client=None):\n",
    "    \"\"\"Return categories list from disk or from Foursquare website using\n",
    "    client\"\"\"\n",
    "    if client is None:\n",
    "        raw_cats = p.load_var('raw_categories')['categories']\n",
    "    else:\n",
    "        raw_cats = client.venues.categories()\n",
    "        p.save_var('raw_categories', raw_cats)\n",
    "        raw_cats = raw_cats['categories']\n",
    "    cats = Category('1', 'Venue', 0, parse_categories(raw_cats))\n",
    "    # pylint: disable=E1101\n",
    "    id_index = [(id_, idx + 100)\n",
    "                for idx, id_ in enumerate(sorted(CAT_TO_ID.values()))\n",
    "                if id_ not in ['0', '1']]\n",
    "    ID_TO_INDEX.update(id_index)\n",
    "    return cats\n",
    "\n",
    "\n",
    "CATS = globals()['get_categories']()\n",
    "\n",
    "\n",
    "def search_categories(query, cats=CATS, field=None):\n",
    "    \"\"\"Return a category matching query (either by name or id) and its path\n",
    "    inside cats.\"\"\"\n",
    "    if field is None:\n",
    "        field = choose_type(query)\n",
    "    if cats[field] == query:\n",
    "        return cats, [query]\n",
    "    for sub_category in cats.sub:\n",
    "        found, path = search_categories(query, sub_category, field)\n",
    "        if found is not None:\n",
    "            return found, [cats[field]] + path\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def choose_type(query):\n",
    "    \"\"\"Return appropriate field index for `query`.\"\"\"\n",
    "    if query[0] in string.digits:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def pre_traversal(cats, field):\n",
    "    \"\"\"Return a flat list of `field` by performing a depth first traversal of\n",
    "    `cats`, visiting the root first.\"\"\"\n",
    "    assert field in [0, 1]\n",
    "    if not cats.sub:\n",
    "        return [cats[field]]\n",
    "    all_subs = [pre_traversal(sub, field) for sub in cats.sub]\n",
    "    return [cats[field]] + [s for sub in all_subs for s in sub]\n",
    "\n",
    "\n",
    "def json_traversal(cats, field):\n",
    "    \"\"\".\"\"\"\n",
    "    if not cats.sub:\n",
    "        return {'name': cats[field]}\n",
    "    all_subs = [json_traversal(sub, field) for sub in cats.sub]\n",
    "    return {'name': cats[field],\n",
    "            'children': all_subs}\n",
    "\n",
    "\n",
    "def get_subcategories(query, field=None):\n",
    "    \"\"\"Return a list of `query` and all its sub categories\"\"\"\n",
    "    root, _ = search_categories(query)\n",
    "    field = choose_type(query) if not field else field.value - 1\n",
    "    return pre_traversal(root, field)\n",
    "\n",
    "\n",
    "cbar, bpath = search_categories('Bar')\n",
    "all_college = get_subcategories('4d4b7105d754a06372d81259', Field.id)\n",
    "# print(all_college)\n",
    "j = json_traversal(search_categories('1')[0], 1)\n",
    "import json\n",
    "import codecs\n",
    "with codecs.open('flare.json', 'w', 'utf8') as f:\n",
    "    json.dump(j, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Processing\n",
    "This main step is to transform the raw data collected into a feature matrix whose rows are each venue of a city with enough visits and column are the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils.py/memodict\n",
      "utils.py/memodict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"import arguments\\ncity = 'moscow'\\nDB, CLIENT = cm.connect_to_db('foursquare', '127.0.0.1', '27017')\\ndo_cluster = lambda val, k: cluster.kmeans2(val, k, 20, minit='points')\\n\\ndef getclass(c, kl, visits):\\n    #Return {id: time pattern} of the venues in class `c` of\\n    #`kl`.\\n    return {v[0]: v[1] for v, k in zip(visits.iteritems(), kl) if k == c}\\n\\ndef peek_at_class(c, kl, visits, k=15):\\n    #Return a table of `k` randomly chosen venues in class `c` of\\n    #`kl`.\\n    sample = r.sample([get_venue(i)\\n                       for i in getclass(c, kl, visits).keys()], k)\\n    return pd.DataFrame({'cat': [_[0] for _ in sample],\\n                         'name': [_[1] for _ in sample],\\n                         'id': [_[2] for _ in sample]})\\nfor c in cm.cities.SHORT_KEY:\\n    if c == 'newyork':\\n        continue\\n    describe_city(c)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Try to describe venue by various features.\"\"\"\n",
    "import prettyplotlib as ppl\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import CommonMongo as cm\n",
    "#import FSCategories as fsc\n",
    "#import explore as xp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils as u\n",
    "import random as r\n",
    "import itertools\n",
    "import scipy.io as sio\n",
    "import scipy.cluster.vq as cluster\n",
    "from scipy.stats import multivariate_normal\n",
    "import re\n",
    "import string\n",
    "import persistent as p\n",
    "import Surrounding as s\n",
    "NOISE = re.compile(r'[\\s'+string.punctuation+r']')\n",
    "DB = None\n",
    "CLIENT = None\n",
    "LEGEND = 'v^<>s*xo|8d+'\n",
    "CATS = ['Arts & Entertainment', 'College & University', 'Food',\n",
    "        'Nightlife Spot', 'Outdoors & Recreation', 'Shop & Service',\n",
    "        'Professional & Other Places', 'Residence', 'Travel & Transport']\n",
    "RADIUS = 350\n",
    "SMOOTH = multivariate_normal([0, 0], (RADIUS/2.5)*np.eye(2))\n",
    "SMOOTH_MAX = SMOOTH.pdf([0, 0])\n",
    "\n",
    "\n",
    "def geo_project(city, entities):\n",
    "    \"\"\"Return {id: euclidean projection in `city`} for objects in\n",
    "    `entities`.\"\"\"\n",
    "    ids, loc = zip(*[(_['_id'], list(reversed(_['loc']['coordinates'])))\n",
    "                     for _ in entities])\n",
    "    project = cm.cities.GEO_TO_2D[city]\n",
    "    return dict(zip(ids, project(np.array(loc))))\n",
    "\n",
    "\n",
    "@u.memodict\n",
    "def is_event(cat_id):\n",
    "    \"\"\"Does `cat_id` represent an event.\"\"\"\n",
    "    return cat_id in fsc.get_subcategories('Event', fsc.Field.id)\n",
    "\n",
    "\n",
    "def global_info(city, standalone=False):\n",
    "    \"\"\"Gather global statistics about `city`.\"\"\"\n",
    "    lvenues = geo_project(city, DB.venue.find({'city': city}, {'loc': 1}))\n",
    "    lcheckins = geo_project(city, DB.checkin.find({'city': city}, {'loc': 1}))\n",
    "    lphotos = geo_project(city, CLIENT.world.photos.find({'hint': city},\n",
    "                                                         {'loc': 1}))\n",
    "    local_projection = [lvenues, lcheckins, lphotos]\n",
    "    visits = xp.get_visits(CLIENT, xp.Entity.venue, city)\n",
    "    visitors = xp.get_visitors(CLIENT, city)\n",
    "    density = estimate_density(city)\n",
    "    activity = [visits, visitors, density]\n",
    "    global TOP_CATS\n",
    "    TOP_CATS = p.load_var('top_cats.my')\n",
    "    infos = {'venue': [] if standalone else ['cat', 'cats'],\n",
    "             'photo': ['taken'] if standalone else ['venue']}\n",
    "    svenues = s.Surrounding(DB.venue, {'city': city}, infos['venue'], lvenues)\n",
    "    scheckins = s.Surrounding(DB.checkin, {'city': city}, ['time'], lcheckins)\n",
    "    sphotos = s.Surrounding(CLIENT.world.photos, {'hint': city},\n",
    "                            infos['photo'], lphotos)\n",
    "    surroundings = [svenues, scheckins, sphotos]\n",
    "    p.save_var('{}_s{}s.my'.format(city, 'venue'), svenues)\n",
    "    if standalone:\n",
    "        for name, var in zip(['venue', 'checkin', 'photo'], surroundings):\n",
    "            p.save_var('{}_s{}s.my'.format(city, name), var)\n",
    "    return local_projection + activity + surroundings\n",
    "\n",
    "\n",
    "def describe_city(city):\n",
    "    \"\"\"Compute feature vector for selected venue in `city`.\"\"\"\n",
    "    CATS2 = p.load_var('cat_depth_2.my')\n",
    "    # a few venues don't have level 2 categories (TODO add it manually?)\n",
    "    CATS2.update({cat: int(idx*1e5) for idx, cat in enumerate(CATS)})\n",
    "    info = global_info(city)\n",
    "    lvenues, lcheckins, lphotos = info[:3]\n",
    "    visits, visitors, density = info[3:6]\n",
    "    nb_visitors = np.unique(np.array([v for place in visitors.itervalues()\n",
    "                                      for v in place])).size\n",
    "    svenues, scheckins, sphotos = info[6:]\n",
    "    categories = categories_repartition(city, svenues, lvenues, RADIUS)\n",
    "    venues = DB.venue.find({'city': city, 'closed': {'$ne': True},\n",
    "                            'cat': {'$ne': None}, 'usersCount': {'$gt': 1}},\n",
    "                           {'cat': 1})\n",
    "    chosen = [v['_id'] for v in venues\n",
    "              if len(visits.get(v['_id'], [])) > 4 and\n",
    "              len(np.unique(visitors.get(v['_id'], []))) > 1 and\n",
    "              not is_event(v['cat'])]\n",
    "    print(\"Chosen {} venues in {}.\".format(len(chosen), city))\n",
    "    info, _ = venues_info(chosen, visits, visitors, density, depth=2,\n",
    "                          tags_freq=False)\n",
    "    print(\"{} of them will be in the matrix.\".format(len(info)))\n",
    "    numeric = np.zeros((len(info), 31), dtype=np.float32)\n",
    "    numeric[:, :5] = np.array([info['likes'], info['users'], info['checkins'],\n",
    "                               info['H'], info['Den']]).T\n",
    "    print('venues with no level 2 category:')\n",
    "    print([info.index[i] for i, c in enumerate(info['cat'])\n",
    "           if CATS2[c] % int(1e5) == 0])\n",
    "    numeric[:, 5] = [CATS2[c] for c in info['cat']]\n",
    "    numeric[:, 24] = np.array(info['Ht'])\n",
    "    for idx, vid in enumerate(info.index):\n",
    "        surrounding = full_surrounding(vid, lvenues, lphotos, lcheckins,\n",
    "                                       svenues, scheckins, sphotos, city)\n",
    "        cat, focus, ratio, around_visits = surrounding\n",
    "        numeric[idx, 6:15] = cat\n",
    "        numeric[idx, 15] = focus\n",
    "        numeric[idx, 16] = ratio\n",
    "        own_visits = visits[vid]\n",
    "        numeric[idx, 17] = is_week_end_place(own_visits)\n",
    "        daily_visits = xp.aggregate_visits(own_visits, 1, 4)[0]\n",
    "        numeric[idx, 18:24] = xp.to_frequency(daily_visits)\n",
    "        numeric[idx, 25:31] = xp.to_frequency(around_visits)\n",
    "    weird = np.argwhere(np.logical_or(np.isnan(numeric), np.isinf(numeric)))\n",
    "    numeric[weird] = 0.0\n",
    "    sio.savemat(city+'_fv', {'v': numeric, 'c': categories,\n",
    "                             'i': np.array(list(info.index)),\n",
    "                             'stat': [nb_visitors]}, do_compression=True)\n",
    "\n",
    "\n",
    "def venues_info(vids, visits=None, visitors=None, density=None, depth=10,\n",
    "                tags_freq=True):\n",
    "    \"\"\"Return various info about from the venue ids `vids`.\"\"\"\n",
    "    tags = defaultdict(int)\n",
    "    city = DB.venue.find_one({'_id': vids[0]})['city']\n",
    "    visits = visits or xp.get_visits(CLIENT, xp.Entity.venue, city)\n",
    "    visitors = visitors or xp.get_visitors(CLIENT, city)\n",
    "    density = density or estimate_density(city)\n",
    "    venues = list(DB.venue.find({'_id': {'$in': vids}},\n",
    "                                {'cat': 1, 'name': 1, 'loc': 1,\n",
    "                                 'price': 1, 'rating': 1, 'tags': 1,\n",
    "                                 'likes': 1, 'usersCount': 1,\n",
    "                                 'checkinsCount': 1}))\n",
    "\n",
    "    msg = 'Asked for {} but get only {}'.format(len(vids), len(venues))\n",
    "    assert len(vids) == len(venues), msg\n",
    "    res = pd.DataFrame(index=[_['_id'] for _ in venues])\n",
    "\n",
    "    def add_col(field):\n",
    "        res[field.replace('Count', '')] = [_[field] for _ in venues]\n",
    "    for field in ['name', 'price', 'rating', 'likes',\n",
    "                  'usersCount', 'checkinsCount']:\n",
    "        add_col(field)\n",
    "    if tags_freq:\n",
    "        res['tags'] = [[normalized_tag(t) for t in _['tags']] for _ in venues]\n",
    "    loc = [_['loc']['coordinates'] for _ in venues]\n",
    "    get_cat = lambda c, d: top_category(c) if d == 1 else parenting_cat(c, d)\n",
    "    res['cat'] = [get_cat(_['cat'], depth) for _ in venues]\n",
    "    res['vis'] = [len(visits[id_]) for id_ in res.index]\n",
    "    res['H'] = [venue_entropy(visitors[id_]) for id_ in res.index]\n",
    "    res['Ht'] = [time_entropy(visits[id_]) for id_ in res.index]\n",
    "    coords = np.fliplr(np.array(loc))\n",
    "    points = cm.cities.GEO_TO_2D[city](coords)\n",
    "    res['Den'] = density(points)\n",
    "    if tags_freq:\n",
    "        for venue in venues:\n",
    "            for tag in venue['tags']:\n",
    "                tags[normalized_tag(tag)] += 1\n",
    "    return res, OrderedDict(sorted(tags.iteritems(), key=lambda x: x[1],\n",
    "                                   reverse=True))\n",
    "\n",
    "\n",
    "def estimate_density(city):\n",
    "    \"\"\"Return a Gaussian KDE of venues in `city`.\"\"\"\n",
    "    kde = KernelDensity(bandwidth=175, rtol=1e-4)\n",
    "    surround = xp.build_surrounding(DB.venue, city, likes=-1, checkins=1)\n",
    "    kde.fit(surround.venues[:, :2])\n",
    "    max_density = approximate_maximum_density(kde, surround.venues[:, :2])\n",
    "    # pylint: disable=E1101\n",
    "    return lambda xy: np.exp(kde.score_samples(xy))/max_density\n",
    "\n",
    "\n",
    "def approximate_maximum_density(kde, venues, precision=128):\n",
    "    \"\"\"Evaluate the kernel on a grid and return the max value.\"\"\"\n",
    "    # pylint: disable=E1101\n",
    "    xgrid = np.linspace(np.min(venues[:, 0]), np.max(venues[:, 0]), precision)\n",
    "    ygrid = np.linspace(np.min(venues[:, 1]), np.max(venues[:, 1]), precision)\n",
    "    X, Y = np.meshgrid(xgrid, ygrid)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    estim = np.exp(kde.score_samples(xy))\n",
    "    return estim.max()\n",
    "\n",
    "\n",
    "def smoothed_location(loc, center, radius, city, pmapping):\n",
    "    \"\"\"Return a list of weight (obtained by a 2D Gaussian with `radius`)\n",
    "    corresponding to the relative distance of points in `loc` with\n",
    "    `center`. `pmapping` is a dictionnary {id: 2dpos} and `center` a 2D\n",
    "    point.\"\"\"\n",
    "    if len(loc) == 0:\n",
    "        return []\n",
    "    if len(loc) == 1:\n",
    "        return [1.0]\n",
    "    assert len(center) == 2\n",
    "    # TODO: loc could directly be the subset\n",
    "    ploc = np.array([pmapping[_] for _ in loc]) - center\n",
    "    return SMOOTH.pdf(ploc/20)/SMOOTH_MAX\n",
    "\n",
    "\n",
    "def full_surrounding(vid, vmapping, pmapping, cmapping, svenues, scheckins,\n",
    "                     sphotos, city, radius=350):\n",
    "    \"\"\"Return a list of photos, checkins and venues categories in a `radius`\n",
    "    around `vid`, within `city`. The mappings are dict({id: 2dpos})\"\"\"\n",
    "    cat_distrib = categories_repartition(city, svenues, vmapping, radius, vid)\n",
    "    center = vmapping[vid]\n",
    "    pids, infos, _ = sphotos.around(center, radius)\n",
    "    pvenue = infos[0]\n",
    "    cids, infos, _ = scheckins.around(center, radius)\n",
    "    ctime = infos[0]\n",
    "    focus = photo_focus(vid, center, pids, pvenue, radius, pmapping)\n",
    "    photogeny, c_smoothed = photo_ratio(center, pids, cids, radius, pmapping,\n",
    "                                        cmapping)\n",
    "    if len(ctime) < 5:\n",
    "        print(vid + ' is anomalous because there is less than 5 check-in in a 350m radius')\n",
    "    if len(ctime) == 0:\n",
    "        surround_visits = np.ones(6)\n",
    "    else:\n",
    "        surround_visits = xp.aggregate_visits(ctime, 1, 4, c_smoothed)[0]\n",
    "    return cat_distrib, focus, photogeny, surround_visits\n",
    "\n",
    "\n",
    "def photo_focus(vid, center, pids, pvenue, radius, mapping):\n",
    "    \"\"\"Return the ratio of photos with venue id around `vid` that are indeed\n",
    "    about it.\"\"\"\n",
    "    this_venue = 0\n",
    "    all_venues = 0\n",
    "    smoothed = smoothed_location(pids, center, radius, None, mapping)\n",
    "    for pid, weight in zip(pvenue, smoothed):\n",
    "        if pid:\n",
    "            if pid == vid:\n",
    "                this_venue += weight\n",
    "            else:\n",
    "                all_venues += weight\n",
    "    return 0 if all_venues < 1e-4 else this_venue / all_venues\n",
    "\n",
    "\n",
    "def photo_ratio(center, pids, cids, radius, pmapping, cmapping):\n",
    "    \"\"\"Return nb_photos/nb_checkins around `vid`, weighted by Gaussian.\"\"\"\n",
    "    p_smoothed = smoothed_location(pids, center, radius, None, pmapping)\n",
    "    c_smoothed = smoothed_location(cids, center, radius, None, cmapping)\n",
    "    # sum of c_smoothed ≠ 0 because for the venue to exist, there must be some\n",
    "    # checkins around. NOTE: actually, there are anomalous venues for which it\n",
    "    # is not the case\n",
    "    return np.sum(p_smoothed)/np.sum(c_smoothed), c_smoothed\n",
    "\n",
    "\n",
    "def is_week_end_place(place_visits):\n",
    "    \"\"\"Tell if a place is more visited during the weekend.\"\"\"\n",
    "    is_we_visit = lambda h, d: d == 5 or (d == 4 and h >= 20) or \\\n",
    "        (d == 6 and h <= 20)\n",
    "    we_visits = [1 for v in place_visits if is_we_visit(v.hour, v.weekday())]\n",
    "    return int(len(we_visits) > 0.5*len(place_visits))\n",
    "\n",
    "\n",
    "def categories_repartition(city, svenues, vmapping, radius, vid=None):\n",
    "    \"\"\"Return the distribution of top level Foursquare categories in\n",
    "    `ball` (ie around `vid`) (or the whole `city` without weighting if\n",
    "    None).\"\"\"\n",
    "    smoothed_loc = itertools.cycle([1.0])\n",
    "    if vid:\n",
    "        vids, vcats, _ = svenues.around(vmapping[vid], radius)\n",
    "        smoothed_loc = smoothed_location(vids, vmapping[vid], radius, city,\n",
    "                                         vmapping)\n",
    "    else:\n",
    "        vids, vcats, _ = svenues.all()\n",
    "    vcats = vcats[0]\n",
    "    distrib = defaultdict(int)\n",
    "    for own_cat, weight in zip(vcats, smoothed_loc):\n",
    "        for cat in own_cat:\n",
    "            distrib[TOP_CATS[cat]] += weight\n",
    "    distrib = np.array([distrib[c] for c in CATS])\n",
    "    # Can't be zero because there is always at least the venue itself in\n",
    "    # surrounding.\n",
    "    return distrib / np.sum(distrib)\n",
    "\n",
    "\n",
    "def venue_entropy(visitors):\n",
    "    \"\"\"Compute the entropy of venue given the list of its `visitors`.\"\"\"\n",
    "    # pylint: disable=E1101\n",
    "    return u.compute_entropy(np.array(Counter(visitors).values(), dtype=float))\n",
    "\n",
    "\n",
    "def time_entropy(visits):\n",
    "    \"\"\"Compute entropy of venue with respect to time of the day of its\n",
    "    checkins.\"\"\"\n",
    "    hours = np.bincount([t.hour for t in visits], minlength=24)\n",
    "    return u.compute_entropy(hours.astype(float))/np.log(24.0)\n",
    "\n",
    "\n",
    "def normalized_tag(tag):\n",
    "    \"\"\"normalize `tag` by removing punctuation and space character.\"\"\"\n",
    "    return NOISE.sub('', tag).lower()\n",
    "\n",
    "\n",
    "def count_tags(tags):\n",
    "    \"\"\"Count occurence of a list of list of tags.\"\"\"\n",
    "    return Counter([normalized_tag(t) for oneset in tags for t in oneset])\n",
    "\n",
    "\n",
    "@u.memodict\n",
    "def top_category(cat):\n",
    "    return parenting_cat(cat, 1)\n",
    "\n",
    "\n",
    "def parenting_cat(cat, depth=1):\n",
    "    \"\"\"Return the name of category id `cat` (or name), stopping at level\n",
    "    `depth`.\"\"\"\n",
    "    if not cat:\n",
    "        return None\n",
    "    _, path = fsc.search_categories(cat)\n",
    "    cat_is_name = fsc.choose_type(cat)\n",
    "    answer = path[depth] if len(path) > depth else path[-1]\n",
    "    if cat_is_name:\n",
    "        return answer\n",
    "    return fsc.CAT_TO_ID[:answer]\n",
    "\n",
    "\n",
    "def get_loc(vid):\n",
    "    \"\"\"Return coordinated of the venue `vid` (or None if it's not in DB).\"\"\"\n",
    "    res = DB.venue.find_one({'_id': vid}, {'loc': 1})\n",
    "    if res:\n",
    "        return u.get_nested(res, ['loc', 'coordinates'])\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_venue(vid, depth=1):\n",
    "    \"\"\"Return a textual description of venue `vid` or None.\"\"\"\n",
    "    venue = DB.venue.find_one({'_id': vid}, {'cat': 1, 'name': 1})\n",
    "    if not venue:\n",
    "        return None\n",
    "    cat = parenting_cat(venue.get('cat'), depth)\n",
    "    venue['cat'] = cat or '???'\n",
    "    return (venue['cat'], venue['name'], vid)\n",
    "\n",
    "\n",
    "def photos_around(id_, centroid, offset, daily, radius=200):\n",
    "    \"\"\"Gather photos timestamp in a `radius` around `id_` and return its time\n",
    "    pattern (`daily` or not), and its distance to every `centroid`.\"\"\"\n",
    "    center = get_loc(id_)\n",
    "    photos = xp.get_visits(CLIENT, xp.Entity.photo, ball=(center, radius))\n",
    "    kind = xp.to_frequency(xp.aggregate_visits(photos.values(), offset)[daily])\n",
    "    nb_class = centroid.shape[0]\n",
    "    # pylint: disable=E1101\n",
    "    classes = np.linalg.norm(np.tile(kind, (nb_class, 1)) - centroid, axis=1)\n",
    "    return len(photos), kind, classes, np.argmin(classes)\n",
    "\n",
    "\n",
    "def named_ticks(kind, offset=0, chunk=3):\n",
    "    \"\"\"Return ticks label for kind in ('day', 'week', 'mix').\"\"\"\n",
    "    if kind is 'day':\n",
    "        period = lambda i: '{}--{}'.format(i % 24, (i+chunk) % 24)\n",
    "        return [period(i) for i in range(0+offset, 24+offset, chunk)]\n",
    "    days = 'mon tue wed thu fri sat sun'.split()\n",
    "    if kind is 'week':\n",
    "        return days\n",
    "    if kind is 'mix':\n",
    "        period = '1 2 3'.split()\n",
    "        return [d+''+p for d in days for p in period]\n",
    "    raise ValueError('`kind` argument is not valid')\n",
    "\n",
    "\n",
    "def draw_classes(centroid, offset, chunk=3):\n",
    "    \"\"\"Plot each time patterns in `centroid`.\"\"\"\n",
    "    size = centroid.shape[0]\n",
    "    for i, marker in zip(range(size), LEGEND[:size]):\n",
    "        ppl.plot(centroid[i, :], marker+'-', ms=9, c=ppl.colors.set1[i])\n",
    "    if centroid.shape[1] == 24/chunk:\n",
    "        plt.xticks(range(24/chunk), named_ticks('day', offset, chunk))\n",
    "    else:\n",
    "        plt.xticks(range(7*3), named_ticks('mix'))\n",
    "\n",
    "\n",
    "def get_distorsion(ak, kl, sval):\n",
    "    \"\"\"Compute the sum of euclidean distance from `sval` to its\n",
    "    centroid\"\"\"\n",
    "    return np.sum(np.linalg.norm(ak[kl, :] - sval, axis=1))\n",
    "\n",
    "\n",
    "import arguments\n",
    "city = 'moscow'\n",
    "DB, CLIENT = cm.connect_to_db('foursquare', '127.0.0.1', '27017')\n",
    "do_cluster = lambda val, k: cluster.kmeans2(val, k, 20, minit='points')\n",
    "\n",
    "def getclass(c, kl, visits):\n",
    "    #Return {id: time pattern} of the venues in class `c` of\n",
    "    #`kl`.\n",
    "    return {v[0]: v[1] for v, k in zip(visits.iteritems(), kl) if k == c}\n",
    "\n",
    "def peek_at_class(c, kl, visits, k=15):\n",
    "    #Return a table of `k` randomly chosen venues in class `c` of\n",
    "    #`kl`.\n",
    "    sample = r.sample([get_venue(i)\n",
    "                       for i in getclass(c, kl, visits).keys()], k)\n",
    "    return pd.DataFrame({'cat': [_[0] for _ in sample],\n",
    "                         'name': [_[1] for _ in sample],\n",
    "                         'id': [_[2] for _ in sample]})\n",
    "for c in cm.cities.SHORT_KEY:\n",
    "    if c == 'newyork':\n",
    "        continue\n",
    "    describe_city(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Computation\n",
    "\n",
    "Defines query_in_one_city, which performs a single similarity query between a GeoJSON polygon from one city to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading city info\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'houston'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7945d0e00196>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mcities_kdtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcities\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mvids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcities_venues_raw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcity\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mvindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcities_desc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcity\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mcities_venues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcity\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'houston'"
     ]
    }
   ],
   "source": [
    "\"\"\"Find closest region in every other cities in the world.\"\"\"\n",
    "import cities as c\n",
    "import json\n",
    "import itertools\n",
    "from scipy.spatial import cKDTree, ConvexHull\n",
    "import approx_emd as app\n",
    "import numpy as np\n",
    "import neighborhood as nb\n",
    "import persistent as p\n",
    "import shapely.geometry as sgeo\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "# load venues location for all cities\n",
    "print('start loading city info')\n",
    "cities = set(c.SHORT_KEY)\n",
    "cities_venues_raw = {name: p.load_var(name+'_svenues.my')\n",
    "                     for name in cities}\n",
    "cities_desc = {name: nb.cn.gather_info(name, raw_features=True,\n",
    "                                       hide_category=True) for name in cities}\n",
    "cities_venues = {}\n",
    "cities_index = {}\n",
    "cities_kdtree = {}\n",
    "for city in cities:\n",
    "    vids, _, locs = cities_venues_raw[city].all()\n",
    "    vindex = cities_desc[city]['index']\n",
    "    cities_venues[city] = np.zeros((len(vindex), 2))\n",
    "    cities_index[city] = dict(itertools.imap(lambda x: (x[1], x[0]),\n",
    "                                             enumerate(vindex)))\n",
    "    for vid, loc in itertools.izip(vids, locs):\n",
    "        pos = cities_index[city].get(vid)\n",
    "        if pos is not None:\n",
    "            cities_venues[city][pos, :] = loc\n",
    "    cities_kdtree[city] = cKDTree(cities_venues[city])\n",
    "print('done')\n",
    "\n",
    "\n",
    "def retrieve_closest_venues(query_venues, query_city, target_city):\n",
    "    \"\"\"For the given query, return a list of venues indices for knn level of\n",
    "    50\"\"\"\n",
    "    mask = np.where(np.in1d(cities_desc[query_city]['index'], query_venues))[0]\n",
    "    query_features = cities_desc[query_city]['features'][mask, :]\n",
    "    all_target_features = cities_desc[target_city]['features']\n",
    "    tindex = cities_desc[target_city]['index']\n",
    "    candidates = app.get_candidates_venues(query_features,\n",
    "                                           all_target_features, k=60)\n",
    "    threshold = int(len(tindex)*1.0*len(query_venues) /\n",
    "                    len(cities_desc[query_city]['index']))\n",
    "    return candidates, threshold\n",
    "\n",
    "\n",
    "def query_in_one_city(source, target, region):\n",
    "    \"\"\"`source` and `target` are two cities name while `region` is a JSON\n",
    "    polygon. Return the five polygon in `target` that are the closest to\n",
    "    `region` according to approximate EMD metrics.\"\"\"\n",
    "    raw_result = []\n",
    "    infos = nb.interpret_query(source, target, region, 'emd')\n",
    "    _, right, _, regions_distance, vids, _ = infos\n",
    "    vindex = np.array(right['index'])\n",
    "    vloc = cities_venues[target]\n",
    "    infos = retrieve_closest_venues(vids, source, target)\n",
    "    candidates, _ = infos\n",
    "    print(source, target)\n",
    "\n",
    "    eps, mpts = 250, 10 if len(vloc) < 5000 else 40\n",
    "    clusters = app.good_clustering(vloc, list(sorted(candidates)), eps, mpts)\n",
    "    areas = []\n",
    "    for cluster in clusters:\n",
    "        venues_areas = app.cluster_to_venues(cluster, vloc,\n",
    "                                             cities_kdtree[target], 4)\n",
    "        if len(venues_areas) == 0:\n",
    "            continue\n",
    "        for venues in venues_areas:\n",
    "            vids = vindex[venues]\n",
    "            venues = right['features'][venues, :]\n",
    "            dst = regions_distance(venues.tolist(),\n",
    "                                   nb.weighting_venues(venues[:, 1]))\n",
    "            areas.append({'venues': set(vids), 'dst': dst})\n",
    "    res = [a['dst'] for a in areas]\n",
    "    venues_so_far = set()\n",
    "    for idx in np.argsort(res):\n",
    "        cand = set(areas[idx]['venues'])\n",
    "        if not venues_so_far.intersection(cand):\n",
    "            venues_so_far.update(cand)\n",
    "        else:\n",
    "            continue\n",
    "        raw_result.append(areas[idx])\n",
    "        if len(raw_result) >= 5:\n",
    "            break\n",
    "    return raw_result\n",
    "\n",
    "\n",
    "def venues_to_geojson(vids, city):\n",
    "    \"\"\"Convert a list of venues id into a GeoJSON polygon\"\"\"\n",
    "    mask = itemgetter(*vids)(cities_index[city])\n",
    "    locs = cities_venues[city][mask, :]\n",
    "    hull = locs[ConvexHull(locs).vertices, :]\n",
    "    geohull = c.euclidean_to_geo(city, hull)\n",
    "    return sgeo.mapping(sgeo.Polygon(np.fliplr(geohull)))\n",
    "\n",
    "\n",
    "with open('thirdworld.json') as inf:\n",
    "    regions = json.load(inf)\n",
    "from collections import namedtuple, defaultdict\n",
    "Query = namedtuple('Query', 'origin targets name geo'.split())\n",
    "queries = []\n",
    "for region in regions:\n",
    "    origin = region['properties']['origin']\n",
    "    targets = cities.difference([origin])\n",
    "    name = region['properties']['name']\n",
    "    geo = region['geometry']\n",
    "    queries.append(Query(origin, targets, name, geo))\n",
    "\n",
    "results = defaultdict(list)\n",
    "for query in queries:\n",
    "    this_query = {}\n",
    "    for city in query.targets:\n",
    "        for res in query_in_one_city(query.origin, city, query.geo):\n",
    "            results[query.name].append((city, res['dst'], res['venues']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
